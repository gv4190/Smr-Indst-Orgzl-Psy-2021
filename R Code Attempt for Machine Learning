########################Step 1: Set up Working Directory#############################

setwd("~/Desktop/Graduate Classes/Winter 2021/Big Data/Machine Learning")
getwd


########################Step 2: Load in CSV##########################################

traindata<-read.csv("train.csv")
test_data<-read.csv("participant_dev.csv")

########################Step 3: Load in necessary packages###########################

library(plyr)
library(caret)
install.packages("mice")
library(mice)
library(dplyr)
install.packages("keras")


########################Step 4: Review necessary information about data###############

  #  This dataset contains 49,102 hires made in a group of retail stores over a period of several
  #  months. Special validation-only performance ratings were collected for a sample (N=12,890)
  #  of these hires. We included data for the entire cohort because part of the task is to create
  #  a predictor of retention; to do this effectively you might choose to use all the data for 
  #  all exits, not just for employees who were retained long enough to have their performance 
  #  evaluated.

colnames(traindata)
tail (traindata)
dim (traindata)
psych::corr.test(traindata[c(2:129)])$r #quick peak at relationships


########################Step 5: Standardize the data##################################

#training set#
traindata[c(12, 15, 18, 21, 24, 27, 30, 33, 36, 45, 54)]<-scale(traindata[c(12, 15, 18, 21, 24, 27, 30, 33, 36, 45, 54)])
#testing set#
test_data[c(4, 7, 10, 13, 16, 19, 22, 25, 28, 37, 46)]<-scale(test_data[c(4, 7, 10, 13, 16, 19, 22, 25, 28, 37, 46)])

############################Step 6: Set up training data##############################


###########Step 6a: Select complete observations (some data within the criterion variables)############

df1<-traindata[!is.na(traindata["High_Performer"]),]

###########Step 6b: Split data into predictor/criterion/fairness variable#############

tcrit<-subset(df1[c("Overall_Rating","Technical_Skills","Teamwork","Customer_Service","Hire_Again","High_Performer","Protected_Group","Retained")])
#tpred<-subset(df1[c(10:129)])
tpred<-read.csv("NEW Imputed Train Predictor Data.csv")
#   orginal code : subset(df1[c(10:129)])
#tpred<-tpred[,-1] #remove uneccesary column created in .csv file 
sum(is.na.data.frame(tpred)) #check for any missing data - after running imputation method (Step 6c)

##################Step 6c: Data Imputation - removed from code (DO NOT RE-RUN - USE CSV FILE#####################

#set.seed(1008)
#tempData<-mice(data = tpred, m = 3, method = "rf", maxit = 3, seed = 1008, where = is.na(tpred)) #random forest imputation
#tpred<-complete(tempData)
#sum(is.na.data.frame(tpred))
   #export to make sure dont have to re-run
#write.csv(tpred, "~/Desktop/Graduate Classes/Winter 2021/Big Data/Machine Learning/NEW Imputed Train Predictor Data.csv")

#############################Step 6e: split train data into training + testing################################

set.seed(1008)
dt = sort(sample(nrow(tpred), nrow(tpred)*.7))
pttrain<-tpred[dt,]
pttest<-tpred[-dt,]

set.seed(1008)
dt2 = sort(sample(nrow(tcrit), nrow(tcrit)*.7))
cttrain<-tcrit[dt2,]
cttest<-tcrit[-dt2,]

####################Step 7: Understanding our Criterion Training Data##############################


#########################Step 7a: Visualize our criterion training variables###############################

  #These are on 5-point scales - removing outliers would be nonsensical
hist(cttrain$Overall_Rating) # normal distribution
hist(cttrain$Technical_Skills) # normal distribution
hist(cttrain$Teamwork) #normal distribution
hist(cttrain$Customer_Service) #normal distribution
hist(cttrain$Hire_Again) # slight left skew - this variable will likely have some good information

#########################Step 7b: Transform Dichotomous Var to Factors (train data) #############################

?make.names

cttrain$High_Performer<- ifelse (cttrain$High_Performer==1, 'High', 'Not.High')
cttrain$High_Performer<- as.factor(cttrain$High_Performer) 
table(cttrain$High_Performer) # 1 = High, 0 = Not High

cttrain$Protected_Group<- ifelse (cttrain$Protected_Group==1, 'Protected.Group', 'Not')
cttrain$Protected_Group<- as.factor(cttrain$Protected_Group) # 1 = Protected Group , 0 = Not
table(cttrain$Protected_Group)

cttrain$Retained<- ifelse (cttrain$Retained==1, 'Retained', 'Termed')
cttrain$Retained<- as.factor(cttrain$Retained) # 1 Retained, 2 = Termed 
table (cttrain$Retained)

#########################Step 7c: Correlation between continuous criterion training variables########################

psych::corr.test(cttrain[c(1:5)])$r #quick peak at relationships
  #                 Overall_Rating Technical_Skills  Teamwork Customer_Service Hire_Again
  # Overall_Rating        1.0000000        0.8423493 0.7964986        0.7348406  0.7534724
  # Technical_Skills      0.8423493        1.0000000 0.7450505        0.6978493  0.6845898
  # Teamwork              0.7964986        0.7450505 1.0000000        0.7739885  0.7040954
  # Customer_Service      0.7348406        0.6978493 0.7739885        1.0000000  0.6497038
  # Hire_Again            0.7534724        0.6845898 0.7040954        0.6497038  1.0000000

####################Step 8: Understanding our Criterion Testing Data##############################


#########################Step 8a: Visualize our testing criterion testing variables###############################

hist(cttest$Overall_Rating) # normal distribution
hist(cttest$Technical_Skills) # normal distribution
hist(cttest$Teamwork) #normal distribution
hist(cttest$Customer_Service) #normal distribution
hist(cttest$Hire_Again) # slight left skew - this variable will likely have some good information

#########################Step 8b: Transform Dichotomous Var to Factors (test data) #############################

##these are the Overall Accuracy outcomes##
cttest$High_Performer <- ifelse (cttest$High_Performer==1, 'High', 'Not_High')
cttest$High_Performer<- as.factor(cttest$High_Performer) 
table(cttest$High_Performer) # 1 = High, 0 = Not High

cttest$Protected_Group<- as.factor(cttest$Protected_Group) # 1 = Protected Group , 0 = Not
table(cttest$Protected_Group)

cttest$Retained<- as.factor(cttest$Retained) # 1 Retained, 2 = Termed 
table (cttest$Retained)

#####################Step 9: Deal with outliers in predictor data##############################




###############Step 10: Perform Elastic Net to find most important variables############

####k folds for ridge####
set.seed(2033) #set seed
fold.ids <- createMultiFolds(cttrain$High_Performer,k=10,times=3) #balances the outcome

HyperGrid <- expand.grid(.alpha= c(seq(.0, .2, by = 0.01)),.lambda=c(seq(.0, .8, by = 0.05)) ) 

fitControl <- trainControl(method = "repeatedcv", 
                           number = 10, 
                           repeats = 3, 
                           index = fold.ids,
                           selectionFunction = "best",  
                           classProbs = T, #allows for other metrics
                           summaryFunction = multiClassSummary, #gives a ton more possible metrics; without, the default is Accuracy/Kappa
                           verboseIter = TRUE)    #gives notification on progress  

en_fit <- train(x = pttrain, y = cttrain$High_Performer , "glmnet", trainControl = fitControl,metric="RMSE", tuneGrid=HyperGrid, standardize= FALSE) #could do ->standardize= TRUE
en_fit #results across tune

en_fit$resample #metrics by each resample (for best tune)

coef(en_fit$finalModel, en_fit$finalModel$lambdaOpt) ### get the weights of the model
ggplot(varImp(en_fit, scale=FALSE)) #varImp differs based on method used in "train", but for lm it is absolute coefficient
varImp(en_fit, scale = FALSE)

#Overall
#Biodata_10     0.03031
#Biodata_04     0.02949
#PScale06_Q3    0.02821
#PScale04_Q3    0.02326
#SJ_Least_8     0.02265
#PScale11_Q4    0.02167
#PScale07_Q2    0.02023
#PScale03_Q4    0.01940
#SJ_Most_6      0.01917
#PScale11_Q3    0.01789
#PScale05_Q1    0.01765
#PScale02_Q1    0.01725
#Scenario1_Time 0.01707
#PScale05_Q4    0.01672
#PScale06_Q2    0.01606
#PScale01_Q4    0.01599
#Scenario2_1    0.01558
#Biodata_08     0.01556
#PScale06_Q1    0.01544
#PScale05_Q2    0.01511

##correlations
test_scores_en <- as.data.frame(predict(en_fit, newdata = pttest))
psych::corr.test(cbind(dplyr::select(cttest, High_Performer), test_scores_en))
######################## Random Forest on High Performers###############################

set.seed(2033) #set
fold.ids <- createMultiFolds(y,k=10,times=1) #balances the outcome

tunegrid <- expand.grid (.mtry= c(2:15)  ) #mtry options#
control <- trainControl(method="repeatedcv", number=10, repeats=1, index = fold.ids, classProbs = T, summaryFunction = multiClassSummary,
                        search="grid", selectionFunction = "best", verboseIter = TRUE)

fit <- train (x = pttrain, y = y, method="rf", ntree=500, nodesize=5,  metric="Accuracy", tuneGrid=tunegrid, trControl=control, standardize=FALSE)
fit
  # mtry  logLoss    AUC        prAUC      Accuracy   Kappa      F1         Sensitivity  Specificity
  # 9    0.6480764  0.6308237  0.6105875  0.6275566  0.1452349  0.3604552  0.2647530    0.8673550  

randomForest::varImpPlot(fit)

OOB_predictions <- predict (fit, newdata = pttest, type="prob") #if you don't list a datafile, it will produce the OOB_predictions
psych::describe (OOB_predictions)

# write.csv(OOB_predictions, "~/Desktop/Graduate Classes/Winter 2021/Big Data/Machine Learning/OOB_Predictions.csv")


######################## Random Forest on Retained###############################

set.seed(2033) #set
fold.ids <- createMultiFolds(cttrain$Retained,k=10,times=1) #balances the outcome

tunegrid <- expand.grid (.mtry= c(2:15)  ) #mtry options#
control <- trainControl(method="repeatedcv", number=10, repeats=1, index = fold.ids, classProbs = T, summaryFunction = multiClassSummary,
                        search="grid", selectionFunction = "best", verboseIter = TRUE)

fit2 <- randomForest::randomForest (cttrain$Retained ~., data= pttrain, method="rf", ntree=500, nodesize=5,  metric="Accuracy", tuneGrid=tunegrid, trControl=control, standardize=FALSE)
fit2 #super high error rate 43.24%


#############################LET'S TRY TPOT#####################################
install.packages("devtools")
library(devtools)

install.packages("mlr")
library(mlr)


devtools::install_github("thllwg/tpotr")
library(tpotr)

install_tpot(method = c("auto", "conda"), conda = "auto",
             version = "default", envname = "r-tpot", extra_packages = NULL,
             extra_pip_packages = NULL, restart_session = TRUE)

##CANNOT INSTALL.... Looking up h2o##
